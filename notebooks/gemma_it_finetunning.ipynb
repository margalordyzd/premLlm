{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed60d7f6",
   "metadata": {},
   "source": [
    "# Fintuning Gemma 3 QAT\n",
    "\n",
    "Adapt a pre-trained model (in our case, the gemma-2-2b-it-4bit model from the MLX Community) to handle function-calling by using the mlx-lm package. \n",
    "\n",
    "This involves creating a specialized chat template, preprocessing a dataset of function call interactions, and applying LoRA for efficient fine-tuning.\n",
    "\n",
    "https://medium.com/@levchevajoana/fine-tuning-a-model-for-function-calling-with-mlx-lm-d00d587e2559"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "79757913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from enum import Enum\n",
    "from typing import Dict, List, Tuple, Union, Any, Dict, List, Optional\n",
    "\n",
    "\n",
    "import mlx.optimizers as optim\n",
    "from datasets import load_dataset\n",
    "from mlx.utils import tree_flatten\n",
    "from mlx_lm import generate, load\n",
    "from mlx_lm.tuner import TrainingArgs, datasets, linear_to_lora_layers, train\n",
    "\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim\n",
    "import numpy as np\n",
    "from mlx.utils import tree_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03721899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d06a8fd6c4472ab237907c45d3d328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the model\n",
    "model_path = \"mlx-community/gemma-3-4b-it-qat-4bit\"\n",
    "model, tokenizer = load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b674315",
   "metadata": {},
   "source": [
    "customize the tokenizer’s chat template to define the structure of our conversational interactions\n",
    "\n",
    "This template embeds special tokens (like `<bos>`, `<start_of_turn>`, `<think>`, and `<tool_call>`) that mark the different stages of the conversation — from the user’s prompt to the model’s internal reasoning and eventual function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ac6e0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.chat_template = (\n",
    "    \"{{ bos_token }}\"\n",
    "    \"{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}\"\n",
    "    \"{% for message in messages %}\"\n",
    "    \"{{ '<start_of_turn>' + message['role'] + '\\n' + message['content'] | trim + '<end_of_turn><eos>\\n' }}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d715277b",
   "metadata": {},
   "source": [
    "## Dataset Preparation and Preprocessing\n",
    "\n",
    "We use the dataset Jofthomas/hermes-function-calling-thinking-V1 which contains conversations involving function calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8723799f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversations'],\n",
       "        num_rows: 3570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = \"Jofthomas/hermes-function-calling-thinking-V1\"\n",
    "\n",
    "dataset = load_dataset(dataset_path)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61e2a832",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename_column(\"conversations\", \"messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff13e684",
   "metadata": {},
   "outputs": [],
   "source": [
    "first = dataset['train'][\"messages\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a3b584a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions.Here are the available tools:<tools> [{'type': 'function', 'function': {'name': 'get_stock_price', 'description': 'Get the current stock price of a company', 'parameters': {'type': 'object', 'properties': {'company': {'type': 'string', 'description': 'The name of the company'}}, 'required': ['company']}}}, {'type': 'function', 'function': {'name': 'get_movie_details', 'description': 'Get details about a movie', 'parameters': {'type': 'object', 'properties': {'title': {'type': 'string', 'description': 'The title of the movie'}}, 'required': ['title']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\\n<tool_call>\\n{tool_call}\\n</tool_call>\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first[0][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6feaf06e",
   "metadata": {},
   "source": [
    "## Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58d7e67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sample):\n",
    "    messages = sample[\"messages\"]\n",
    "    first_message = messages[0]\n",
    "\n",
    "    # Instead of adding a system message, we merge the content into the first user message\n",
    "    if first_message[\"role\"] == \"system\":\n",
    "        system_message_content = first_message[\"content\"]\n",
    "        # Merge system content with the first user message\n",
    "        messages[1][\"content\"] = (\n",
    "            system_message_content\n",
    "            + \"Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\\n\\n\"\n",
    "            + messages[1][\"content\"]\n",
    "        )\n",
    "        # Remove the system message from the conversation\n",
    "        messages.pop(0)\n",
    "\n",
    "    return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False), \"1\": str(len(messages))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58a3b752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "530103003f0d49538f5d716974f1c1db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', '1'],\n",
       "        num_rows: 3213\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', '1'],\n",
       "        num_rows: 357\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.map(preprocess, remove_columns=\"messages\")\n",
    "dataset = dataset[\"train\"].train_test_split(0.1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94fd3e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos><start_of_turn>human\\nYou are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don\\'t make assumptions about what values to plug into functions.Here are the available tools:<tools> [{\\'type\\': \\'function\\', \\'function\\': {\\'name\\': \\'get_movie_details\\', \\'description\\': \\'Get details of a movie based on its title\\', \\'parameters\\': {\\'type\\': \\'object\\', \\'properties\\': {\\'title\\': {\\'type\\': \\'string\\', \\'description\\': \\'The title of the movie\\'}}, \\'required\\': [\\'title\\']}}}, {\\'type\\': \\'function\\', \\'function\\': {\\'name\\': \\'calculate_tip\\', \\'description\\': \\'Calculate the amount of tip based on a bill\\', \\'parameters\\': {\\'type\\': \\'object\\', \\'properties\\': {\\'bill_amount\\': {\\'type\\': \\'number\\', \\'description\\': \\'The total amount of the bill\\'}, \\'tip_percentage\\': {\\'type\\': \\'number\\', \\'description\\': \\'The percentage of tip to be added\\'}}, \\'required\\': [\\'bill_amount\\', \\'tip_percentage\\']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {\\'title\\': \\'FunctionCall\\', \\'type\\': \\'object\\', \\'properties\\': {\\'arguments\\': {\\'title\\': \\'Arguments\\', \\'type\\': \\'object\\'}, \\'name\\': {\\'title\\': \\'Name\\', \\'type\\': \\'string\\'}}, \\'required\\': [\\'arguments\\', \\'name\\']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\\n<tool_call>\\n{tool_call}\\n</tool_call>Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\\n\\nCan you tell me about the movie \"Inception\"?<end_of_turn><eos>\\n<start_of_turn>model\\n<think>Okay, the user is asking about the movie \"Inception.\" I need to figure out how to respond. I remember that there\\'s a function called get_movie_details which can fetch movie information. It requires the title as a parameter. Since the user provided the title, I should call this function to get the details they\\'re looking for. That makes sense because it directly answers their query using the available tool.\\n</think><tool_call>\\n{\\'name\\': \\'get_movie_details\\', \\'arguments\\': {\\'title\\': \\'Inception\\'}}\\n</tool_call><end_of_turn><eos>\\n<start_of_turn>tool\\n<tool_response>\\n{\\'title\\': \\'Inception\\', \\'director\\': \\'Christopher Nolan\\', \\'release_year\\': 2010, \\'genre\\': \\'Science Fiction\\', \\'cast\\': [\\'Leonardo DiCaprio\\', \\'Ellen Page\\', \\'Joseph Gordon-Levitt\\'], \\'rating\\': 8.8}\\n</tool_response><end_of_turn><eos>\\n<start_of_turn>model\\nThe movie \"Inception\" is a Science Fiction film directed by Christopher Nolan. It was released in the year 2010. The main cast includes Leonardo DiCaprio, Ellen Page, and Joseph Gordon-Levitt. The movie has a rating of 8.8.<end_of_turn><eos>\\n<start_of_turn>human\\nThat sounds interesting. Can you tell me about the movie \"The Dark Knight\"?<end_of_turn><eos>\\n<start_of_turn>model\\n<think>Alright, so the user has asked about \"The Dark Knight.\" I remember that there\\'s a function called get_movie_details which can provide details about movies. I need to make sure I use the correct parameters. The function requires the title of the movie as a parameter, and the user has provided \"The Dark Knight.\" I should call this function with the title to retrieve the necessary information. This will help the user get accurate details about the movie they\\'re interested in.\\n</think><tool_call>\\n{\\'name\\': \\'get_movie_details\\', \\'arguments\\': {\\'title\\': \\'The Dark Knight\\'}}\\n</tool_call><end_of_turn><eos>\\n<start_of_turn>tool\\n<tool_response>\\n{\\'title\\': \\'The Dark Knight\\', \\'director\\': \\'Christopher Nolan\\', \\'release_year\\': 2008, \\'genre\\': \\'Action, Crime, Drama\\', \\'cast\\': [\\'Christian Bale\\', \\'Heath Ledger\\', \\'Aaron Eckhart\\'], \\'rating\\': 9.0}\\n</tool_response><end_of_turn><eos>\\n<start_of_turn>model\\n\"The Dark Knight\" is an Action, Crime, Drama film directed by Christopher Nolan. It was released in 2008. The main cast includes Christian Bale, Heath Ledger, and Aaron Eckhart. The movie has a rating of 9.0.<end_of_turn><eos>\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][\"text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f49f2a3",
   "metadata": {},
   "source": [
    "## Training with LoRA adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43864280",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_path = \"adapters_fc\"\n",
    "os.makedirs(adapter_path, exist_ok=True)\n",
    "adapter_config_path = os.path.join(adapter_path, \"adapter_config.json\")\n",
    "adapter_file_path = os.path.join(adapter_path, \"adapters.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17c2ce4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = {\n",
    "    \"num_layers\": 8,\n",
    "    \"lora_parameters\": {\n",
    "        \"rank\": 16,\n",
    "        \"scale\": 64,\n",
    "        \"dropout\": 0.05,\n",
    "    },\n",
    "}\n",
    "with open(adapter_config_path, \"w\") as f:\n",
    "    json.dump(lora_config, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1368245",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArgs(\n",
    "    adapter_file=adapter_file_path,\n",
    "    iters=1,\n",
    "    steps_per_eval=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9833e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the original model parameters\n",
    "_ = model.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fa5a6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert selected linear layers to LoRA layers to make only a small subset of parameters trainable.\n",
    "linear_to_lora_layers(model, lora_config[\"num_layers\"], lora_config[\"lora_parameters\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "358714fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 1048576\n"
     ]
    }
   ],
   "source": [
    "num_train_params = sum(v.size for _, v in tree_flatten(model.trainable_parameters()))\n",
    "print(f\"Number of trainable parameters: {num_train_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51ec5da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activate training mode while still preserving the frozen state of the main model parameters.\n",
    "_ = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3383b672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure a metrics tracker to log both training and validation losses\n",
    "\n",
    "\n",
    "def loss(model, inputs, targets, lengths):\n",
    "    # Run model on inputs\n",
    "    logits, _ = model(inputs)\n",
    "    logits = logits.astype(mx.float32)\n",
    "\n",
    "    # Mask padding tokens\n",
    "    length_mask = mx.arange(inputs.shape[1])[None, :] < lengths[:, None]\n",
    "\n",
    "    # Calculate the loss\n",
    "    ce = nn.losses.cross_entropy(logits, targets) * length_mask\n",
    "    ntoks = length_mask.sum()\n",
    "    ce = ce.sum() / ntoks\n",
    "    return ce, ntoks\n",
    "\n",
    "class Metrics:\n",
    "    def __init__(self) -> None:\n",
    "        self.train_losses: List[Tuple[int, float]] = []\n",
    "        self.val_losses: List[Tuple[int, float]] = []\n",
    "\n",
    "    def on_train_loss_report(self, info: Dict[str, Union[float, int]]) -> None:\n",
    "        self.train_losses.append((info[\"iteration\"], info[\"train_loss\"]))\n",
    "\n",
    "    def on_val_loss_report(self, info: Dict[str, Union[float, int]]) -> None:\n",
    "        self.val_losses.append((info[\"iteration\"], info[\"val_loss\"]))\n",
    "\n",
    "\n",
    "metrics = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "572dc41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mlx-lm–suitable datasets by first defining the following configuration about our datasets\n",
    "\n",
    "configs = {\n",
    "    \"mask_prompt\": False,\n",
    "    \"prompt_feature\": \"prompt\",\n",
    "    \"text_feature\": \"text\",\n",
    "    \"completion_feature\": \"completion\",\n",
    "    \"chat_feature\": \"messages\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a77b773",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = datasets.create_dataset(\n",
    "    dataset[\"train\"],\n",
    "    tokenizer,\n",
    "    configs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9351c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = datasets.create_dataset(\n",
    "    dataset[\"test\"],\n",
    "    tokenizer,\n",
    "    configs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c738e52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_imp(model, train_set, val_set, optimizer, loss, tokenizer, args):\n",
    "#     # Create value and grad function for loss\n",
    "#     loss_value_and_grad = nn.value_and_grad(model, loss)\n",
    "\n",
    "#     losses = []\n",
    "#     n_tokens = 0\n",
    "\n",
    "#     # Main training loop\n",
    "#     start = time.perf_counter()\n",
    "#     for it, batch in zip(\n",
    "#         range(args.iters),\n",
    "#         iterate_batches(train_set, tokenizer, args.batch_size, train=True),\n",
    "#     ):\n",
    "#         # Forward and backward pass\n",
    "#         (lvalue, toks), grad = loss_value_and_grad(model, *batch)\n",
    "\n",
    "#         # Model update\n",
    "#         optimizer.update(model, grad)\n",
    "#         mx.eval(model.parameters(), optimizer.state, lvalue)\n",
    "\n",
    "#         # Record loss\n",
    "#         losses.append(lvalue.item())\n",
    "#         n_tokens += toks.item()\n",
    "\n",
    "#         # Report training loss if needed\n",
    "#         if (it + 1) % args.steps_per_report == 0:\n",
    "#             train_loss = np.mean(losses)\n",
    "\n",
    "#             stop = time.perf_counter()\n",
    "#             print(\n",
    "#                 f\"Iter {it + 1}: Train loss {train_loss:.3f}, \"\n",
    "#                 f\"It/sec {args.steps_per_report / (stop - start):.3f}, \"\n",
    "#                 f\"Tokens/sec {float(n_tokens) / (stop - start):.3f}\"\n",
    "#             )\n",
    "#             losses = []\n",
    "#             n_tokens = 0\n",
    "#             start = time.perf_counter()\n",
    "\n",
    "#         # Report validation loss if needed\n",
    "#         if it == 0 or (it + 1) % args.steps_per_eval == 0:\n",
    "#             stop = time.perf_counter()\n",
    "#             val_loss = evaluate(\n",
    "#                 model, val_set, loss, tokenizer, args.batch_size, args.val_batches\n",
    "#             )\n",
    "#             print(\n",
    "#                 f\"Iter {it + 1}: \"\n",
    "#                 f\"Val loss {val_loss:.3f}, \"\n",
    "#                 f\"Val took {(time.perf_counter() - stop):.3f}s\"\n",
    "#             )\n",
    "\n",
    "#             start = time.perf_counter()\n",
    "\n",
    "#         # Save adapter weights if needed\n",
    "#         if (it + 1) % args.save_every == 0:\n",
    "#             mx.savez(\n",
    "#                 args.adapter_file, **dict(tree_flatten(model.trainable_parameters()))\n",
    "#             )\n",
    "#             print(f\"Iter {it + 1}: Saved adapter weights to {args.adapter_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ff9eee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def iterate_batches(dset, tokenizer, batch_size, train=False):\n",
    "#     # Shuffle indices\n",
    "#     while True:\n",
    "#         indices = np.arange(len(dset))\n",
    "#         if train:\n",
    "#             indices = np.random.permutation(indices)\n",
    "\n",
    "#         # Collect batches from dataset\n",
    "#         for i in range(0, len(indices) - batch_size + 1, batch_size):\n",
    "#             # Encode batch\n",
    "#             batch = [tokenizer.encode(dset[indices[i + j]]) for j in range(batch_size)]\n",
    "#             lengths = [len(x) for x in batch]\n",
    "\n",
    "#             # Check if any sequence is longer than 2048 tokens\n",
    "#             if max(lengths) > 2048:\n",
    "#                 print(\n",
    "#                     \"[WARNING] Some sequences are longer than 2048 tokens. \"\n",
    "#                     \"Consider pre-splitting your data to save memory.\"\n",
    "#                 )\n",
    "\n",
    "#             # Pad to the max length\n",
    "#             batch_arr = np.zeros((batch_size, max(lengths)), np.int32)\n",
    "\n",
    "#             for j in range(batch_size):\n",
    "#                 batch_arr[j, : lengths[j]] = batch[j]\n",
    "#             batch = mx.array(batch_arr)\n",
    "#             yield batch[:, :-1], batch[:, 1:], mx.array(lengths)\n",
    "\n",
    "#         if not train:\n",
    "#             break\n",
    "\n",
    "class CacheDataset:\n",
    "    def __init__(self, data: Any):\n",
    "        self._data = data\n",
    "        self._proc_data = [None] * len(data)\n",
    "\n",
    "    def itemlen(self, idx: int):\n",
    "        return len(self._data[idx])\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        if self._proc_data[idx] is None:\n",
    "            self._proc_data[idx] = self._data.process(self._data[idx])\n",
    "        return self._proc_data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07cca52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training..., iters: 1\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Start the fine-tuning process by calling the train() function\n",
    "\n",
    "train(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        optimizer=optim.Adam(learning_rate=1e-5),\n",
    "        train_dataset=CacheDataset(train_set),\n",
    "        val_dataset=CacheDataset(val_set),\n",
    "        training_callback=metrics,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e441d0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/Users/margarito/opt/anaconda3/envs/llm/lib/python3.10/site-packages/mlx_lm/tuner/trainer.py\u001b[0m(94)\u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     92 \u001b[0;31m        \u001b[0mlen_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     93 \u001b[0;31m    \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 94 \u001b[0;31m        \u001b[0mlen_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     95 \u001b[0;31m    \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     96 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "{'text': '<bos><start_of_turn>human\\nYou are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags.You may call one or more functions to assist with the user query. Don\\'t make assumptions about what values to plug into functions.Here are the available tools:<tools> [{\\'type\\': \\'function\\', \\'function\\': {\\'name\\': \\'get_movie_details\\', \\'description\\': \\'Get details of a movie based on its title\\', \\'parameters\\': {\\'type\\': \\'object\\', \\'properties\\': {\\'title\\': {\\'type\\': \\'string\\', \\'description\\': \\'The title of the movie\\'}}, \\'required\\': [\\'title\\']}}}, {\\'type\\': \\'function\\', \\'function\\': {\\'name\\': \\'calculate_tip\\', \\'description\\': \\'Calculate the amount of tip based on a bill\\', \\'parameters\\': {\\'type\\': \\'object\\', \\'properties\\': {\\'bill_amount\\': {\\'type\\': \\'number\\', \\'description\\': \\'The total amount of the bill\\'}, \\'tip_percentage\\': {\\'type\\': \\'number\\', \\'description\\': \\'The percentage of tip to be added\\'}}, \\'required\\': [\\'bill_amount\\', \\'tip_percentage\\']}}}] </tools>Use the following pydantic model json schema for each tool call you will make: {\\'title\\': \\'FunctionCall\\', \\'type\\': \\'object\\', \\'properties\\': {\\'arguments\\': {\\'title\\': \\'Arguments\\', \\'type\\': \\'object\\'}, \\'name\\': {\\'title\\': \\'Name\\', \\'type\\': \\'string\\'}}, \\'required\\': [\\'arguments\\', \\'name\\']}For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\\n<tool_call>\\n{tool_call}\\n</tool_call>Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\\n\\nCan you tell me about the movie \"Inception\"?<end_of_turn><eos>\\n<start_of_turn>model\\n<think>Okay, the user is asking about the movie \"Inception.\" I need to figure out how to respond. I remember that there\\'s a function called get_movie_details which can fetch movie information. It requires the title as a parameter. Since the user provided the title, I should call this function to get the details they\\'re looking for. That makes sense because it directly answers their query using the available tool.\\n</think><tool_call>\\n{\\'name\\': \\'get_movie_details\\', \\'arguments\\': {\\'title\\': \\'Inception\\'}}\\n</tool_call><end_of_turn><eos>\\n<start_of_turn>tool\\n<tool_response>\\n{\\'title\\': \\'Inception\\', \\'director\\': \\'Christopher Nolan\\', \\'release_year\\': 2010, \\'genre\\': \\'Science Fiction\\', \\'cast\\': [\\'Leonardo DiCaprio\\', \\'Ellen Page\\', \\'Joseph Gordon-Levitt\\'], \\'rating\\': 8.8}\\n</tool_response><end_of_turn><eos>\\n<start_of_turn>model\\nThe movie \"Inception\" is a Science Fiction film directed by Christopher Nolan. It was released in the year 2010. The main cast includes Leonardo DiCaprio, Ellen Page, and Joseph Gordon-Levitt. The movie has a rating of 8.8.<end_of_turn><eos>\\n<start_of_turn>human\\nThat sounds interesting. Can you tell me about the movie \"The Dark Knight\"?<end_of_turn><eos>\\n<start_of_turn>model\\n<think>Alright, so the user has asked about \"The Dark Knight.\" I remember that there\\'s a function called get_movie_details which can provide details about movies. I need to make sure I use the correct parameters. The function requires the title of the movie as a parameter, and the user has provided \"The Dark Knight.\" I should call this function with the title to retrieve the necessary information. This will help the user get accurate details about the movie they\\'re interested in.\\n</think><tool_call>\\n{\\'name\\': \\'get_movie_details\\', \\'arguments\\': {\\'title\\': \\'The Dark Knight\\'}}\\n</tool_call><end_of_turn><eos>\\n<start_of_turn>tool\\n<tool_response>\\n{\\'title\\': \\'The Dark Knight\\', \\'director\\': \\'Christopher Nolan\\', \\'release_year\\': 2008, \\'genre\\': \\'Action, Crime, Drama\\', \\'cast\\': [\\'Christian Bale\\', \\'Heath Ledger\\', \\'Aaron Eckhart\\'], \\'rating\\': 9.0}\\n</tool_response><end_of_turn><eos>\\n<start_of_turn>model\\n\"The Dark Knight\" is an Action, Crime, Drama film directed by Christopher Nolan. It was released in 2008. The main cast includes Christian Bale, Heath Ledger, and Aaron Eckhart. The movie has a rating of 9.0.<end_of_turn><eos>\\n', '1': '8'}\n",
      "dict_keys(['text', '1'])\n"
     ]
    }
   ],
   "source": [
    "debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a098d1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_set[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e676698",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
